import os


from langchain.chains import RetrievalQA
from langchain_openai import AzureChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
from langchain_openai import AzureOpenAIEmbeddings


file = 'nba.csv'
loader = CSVLoader(file_path=file)


loader


from langchain.indexes import VectorstoreIndexCreator


get_ipython().getoutput("pip install docarray")


embedding = AzureOpenAIEmbeddings(
    api_key="a4982552aedf4162b7582ce9c31aa977",
    azure_endpoint="https://pcg-east-us-2.openai.azure.com/",  # 从环境变量读取
    openai_api_version="2023-05-15",
    deployment="text-embedding-ada-002-2",
    model="text-embedding-ada-002",  # 使用的模型名称
    validate_base_url=True
)
index = VectorstoreIndexCreator(
    embedding=embedding,  # 添加嵌入模型
    vectorstore_cls=DocArrayInMemorySearch
).from_loaders([loader])


print(os.environ)


variables_to_delete = ['OPENAI_API_VERSION', 
                       'OPENAI_API_KEY',
                       'OPENAI_API_BASE',
                      'AZURE_ENDPOINT']

# 遍历并删除每个环境变量
for var in variables_to_delete:
    os.environ.pop(var, None) 


query='Please list all your favorable players in a table in markdown\
and give a description for each person.'


llm = AzureChatOpenAI(
    api_key="a4982552aedf4162b7582ce9c31aa977",
    azure_endpoint="https://pcg-east-us-2.openai.azure.com/",
    openai_api_version="2024-08-01-preview", 
    deployment_name="gpt-4o-mini",
    model_name="gpt-4o-mini",
    temperature=0.0
)


response=index.query(query,llm=llm)


response


display(Markdown(response))


embed = embedding.embed_query("Hi, my name is Zhian Hu!")


embed


print(len(embed))


"""
现在我们用一种更接近底层的方法来实现，即先加载文件，转化成embeddings加入数据库
接着计算相似度，默认输出前k=4个数据
"""
docs = loader.load()


db = DocArrayInMemorySearch.from_documents(docs, embedding)


query = "Please suggest a player to follow"


docs_new = db.similarity_search(query)


list(docs_new)


retriever = db.as_retriever()


retriever


#第一种方式，直接将所有文件加入prompt传递给大模型

qdocs = "".join(docs_new[i].page_content for i in range(len(docs_new)))

response = llm.invoke(f"{qdocs} Quesiton: Please list players' names and teams\
    in a table in markdown\
    and give a description for each person.")


response


display(Markdown(response.content))


#通过chain来封装上述步骤
qa_stuff = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    verbose=True
)


type(qa_stuff)


query_new = "Please list all players' names in the team 'Boston Celtics'\
    in a table in markdown\
    and give a description for each person."


response_new = qa_stuff.invoke(query_new)


display(Markdown(response_new["result"]))


"""基于文本文件的QA"""
# from langchain_community.document_loaders import TextLoader
# from langchain_community.vectorstores import DocArrayInMemorySearch
# from langchain_openai import OpenAIEmbeddings
# from langchain_text_splitters import CharacterTextSplitter

# # 加载文档
# documents = TextLoader("../../how_to/state_of_the_union.txt").load()

# # 拆分文档
# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
# docs = text_splitter.split_documents(documents)

# # 生成嵌入向量
# embeddings = OpenAIEmbeddings()

# # 创建内存中的向量搜索数据库
# db = DocArrayInMemorySearch.from_documents(docs, embeddings)

# # 相似度检索
# query = "What did the president say about Ketanji Brown Jackson"
# docs = db.similarity_search(query)

# # 打印最相关的文档内容
# print(docs[0].page_content)

